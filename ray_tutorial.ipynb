{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8745aa2",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "Run the below in bash\n",
    "\n",
    "`tensorboard --logdir=./results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ade351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-09-27 16:08:22</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:00.94        </td></tr>\n",
       "<tr><td>Memory:      </td><td>22.2/125.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/24 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  num_training_step_ca\n",
       "lls_per_iteration</th><th style=\"text-align: right;\">       num_env_steps_sample\n",
       "d_lifetime</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_33793_00000</td><td>TERMINATED</td><td>10.0.0.38:263830</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         112.904</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:08:22,606\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/media/ash/Storage/Projects/RL/results/tutorial/PPO_2025-09-27_16-06-21' in 0.0161s.\n",
      "2025-09-27 16:08:23,050\tINFO tune.py:1041 -- Total run time: 121.39 seconds (120.93 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import os\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    # Specify a simple tune hyperparameter sweep.\n",
    "    .training(\n",
    "        num_epochs = 10,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a Tuner instance to manage the trials.\n",
    "tuner = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        storage_path=os.path.abspath(\"./results/tutorial\"),\n",
    "        stop={\"env_runners/episode_return_mean\": 500.0},\n",
    "        checkpoint_config=tune.CheckpointConfig(checkpoint_frequency=10,\n",
    "                                                checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "# Run the Tuner and capture the results.\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34458e2",
   "metadata": {},
   "source": [
    "### Loading a Trained Model (Checkpoint)\n",
    "https://docs.ray.io/en/latest/rllib/getting-started.html#deploy-a-trained-model-for-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f97a2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ray.rllib.core.rl_module import RLModule\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "checkpoint_path = os.path.abspath(\"./results/tutorial/PPO_2025-09-27_16-06-21/PPO_CartPole-v1_33793_00000_0_2025-09-27_16-06-21/checkpoint_000003\")\n",
    "\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    Path(checkpoint_path)\n",
    "    / \"learner_group\"\n",
    "    / \"learner\"\n",
    "    / \"rl_module\"\n",
    "    / \"default_policy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e0d73fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Reset the env to get the initial observation.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m obs, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Uncomment this line to render the env.\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# env.render()\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Compute the next action from a batch (B=1) of observations.\u001b[39;00m\n\u001b[32m     12\u001b[39m     obs_batch = torch.from_numpy(obs).unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# add batch B=1 dimension\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL_env/lib/python3.13/site-packages/gymnasium/wrappers/common.py:146\u001b[39m, in \u001b[36mTimeLimit.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[32m    137\u001b[39m \n\u001b[32m    138\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m    The reset environment\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28mself\u001b[39m._elapsed_steps = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL_env/lib/python3.13/site-packages/gymnasium/core.py:333\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    331\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL_env/lib/python3.13/site-packages/gymnasium/wrappers/common.py:400\u001b[39m, in \u001b[36mOrderEnforcing.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;28mself\u001b[39m._has_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL_env/lib/python3.13/site-packages/gymnasium/core.py:333\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    331\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL_env/lib/python3.13/site-packages/gymnasium/wrappers/common.py:295\u001b[39m, in \u001b[36mPassiveEnvChecker.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m.env, seed=seed, options=options)\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL_env/lib/python3.13/site-packages/gymnasium/envs/classic_control/cartpole.py:244\u001b[39m, in \u001b[36mCartPoleEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mself\u001b[39m.steps_beyond_terminated = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28mself\u001b[39m.state, dtype=np.float32), {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL_env/lib/python3.13/site-packages/gymnasium/envs/classic_control/cartpole.py:334\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    331\u001b[39m gfxdraw.hline(\u001b[38;5;28mself\u001b[39m.surf, \u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.screen_width, carty, (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m    333\u001b[39m \u001b[38;5;28mself\u001b[39m.surf = pygame.transform.flip(\u001b[38;5;28mself\u001b[39m.surf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscreen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    336\u001b[39m     pygame.event.pump()\n",
      "\u001b[31merror\u001b[39m: display Surface quit"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "episode_return = 0.0\n",
    "done = False\n",
    "\n",
    "# Reset the env to get the initial observation.\n",
    "obs, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    # Uncomment this line to render the env.\n",
    "    # env.render()\n",
    "\n",
    "    # Compute the next action from a batch (B=1) of observations.\n",
    "    obs_batch = torch.from_numpy(obs).unsqueeze(0)  # add batch B=1 dimension\n",
    "    model_outputs = rl_module.forward_inference({\"obs\": obs_batch})\n",
    "\n",
    "    # Extract the action distribution parameters from the output and dissolve batch dim.\n",
    "    action_dist_params = model_outputs[\"action_dist_inputs\"][0].numpy()\n",
    "\n",
    "    # # We have continuous actions -> take the mean (max likelihood).\n",
    "    # greedy_action = np.clip(\n",
    "    #     action_dist_params[0:1],  # 0=mean, 1=log(stddev), [0:1]=use mean, but keep shape=(1,)\n",
    "    #     a_min=env.action_space.low[0],\n",
    "    #     a_max=env.action_space.high[0],\n",
    "    # )\n",
    "    # For discrete actions, you should take the argmax over the logits:\n",
    "    greedy_action = np.argmax(action_dist_params)\n",
    "\n",
    "    # Send the action to the environment for the next step.\n",
    "    obs, reward, terminated, truncated, info = env.step(greedy_action)\n",
    "\n",
    "    # Perform env-loop bookkeeping.\n",
    "    episode_return += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "print(f\"Reached episode return of {episode_return}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c4c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
