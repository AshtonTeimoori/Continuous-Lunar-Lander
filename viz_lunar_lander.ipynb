{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8745aa2",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "Run the below in bash\n",
    "\n",
    "`tensorboard --logdir=./results`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34458e2",
   "metadata": {},
   "source": [
    "### Deploy Trained Model For Inference (Checkpoint)\n",
    "https://docs.ray.io/en/latest/rllib/getting-started.html#deploy-a-trained-model-for-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ray.rllib.core.rl_module import RLModule\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7e0d73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.abspath(\"./results/LundarLander/PPO_2025-09-27_16-23-24/PPO_LunarLanderContinuous-v3_96592_00000_0_2025-09-27_16-23-26/checkpoint_000298\")\n",
    "\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    Path(checkpoint_path)\n",
    "    / \"learner_group\"\n",
    "    / \"learner\"\n",
    "    / \"rl_module\"\n",
    "    / \"default_policy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b74ab8",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99d9170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/300: 267.64\n",
      "20/300: 260.72\n",
      "30/300: 258.06\n",
      "40/300: 261.35\n",
      "50/300: 261.16\n",
      "60/300: 260.48\n",
      "70/300: 260.64\n",
      "80/300: 262.80\n",
      "90/300: 262.38\n",
      "100/300: 261.95\n",
      "110/300: 260.67\n",
      "120/300: 261.67\n",
      "130/300: 261.08\n",
      "140/300: 260.97\n",
      "150/300: 261.44\n",
      "160/300: 261.59\n",
      "170/300: 261.42\n",
      "180/300: 261.04\n",
      "190/300: 260.80\n",
      "200/300: 260.80\n",
      "210/300: 260.33\n",
      "220/300: 260.90\n",
      "230/300: 260.92\n",
      "240/300: 260.25\n",
      "250/300: 260.24\n",
      "260/300: 260.19\n",
      "270/300: 260.05\n",
      "280/300: 260.64\n",
      "290/300: 260.89\n",
      "300/300: 260.64\n",
      "Average episode return of 260.64.\n"
     ]
    }
   ],
   "source": [
    "# Create the RL environment to test against (same as was used for training earlier).\n",
    "env = gym.make(\"LunarLander-v3\", gravity=-9.81, wind_power=15.0, enable_wind=False, continuous=True)\n",
    "\n",
    "ave_returns = 0.0\n",
    "trails = 300\n",
    "\n",
    "for i in range(trails):\n",
    "    episode_return = 0.0\n",
    "    done = False\n",
    "\n",
    "    # Reset the env to get the initial observation.\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Uncomment this line to render the env.\n",
    "        # env.render()\n",
    "\n",
    "        # Compute the next action from a batch (B=1) of observations.\n",
    "        obs_batch = torch.from_numpy(obs).unsqueeze(0)  # add batch B=1 dimension\n",
    "        model_outputs = rl_module.forward_inference({\"obs\": obs_batch})\n",
    "\n",
    "        # Extract the action distribution parameters from the output and dissolve batch dim.\n",
    "        action_dist_params = model_outputs[\"action_dist_inputs\"][0].numpy()\n",
    "\n",
    "        # We have continuous actions -> take the mean (max likelihood).\n",
    "        greedy_action_0 = np.clip(\n",
    "            action_dist_params[0],  # 0-1=mean, 2-3=log(stddev)\n",
    "            a_min=env.action_space.low[0],\n",
    "            a_max=env.action_space.high[0],\n",
    "        )\n",
    "        greedy_action_1 = np.clip(\n",
    "            action_dist_params[1],  # 0-1=mean, 2-3=log(stddev)\n",
    "            a_min=env.action_space.low[1],\n",
    "            a_max=env.action_space.high[1],\n",
    "        )\n",
    "        greedy_action = np.array([greedy_action_0, greedy_action_1])\n",
    "        # # For discrete actions, you should take the argmax over the logits:\n",
    "        # greedy_action = np.argmax(action_dist_params)\n",
    "\n",
    "        # Send the action to the environment for the next step.\n",
    "        obs, reward, terminated, truncated, info = env.step(greedy_action)\n",
    "\n",
    "        # Perform env-loop bookkeeping.\n",
    "        episode_return += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    ave_returns += episode_return\n",
    "\n",
    "    if i%10 == 9:\n",
    "        print(str(i+1) + \"/\" + str(trails) + \": \" + str(f\"{ave_returns/(i+1):3.2f}\"))\n",
    "\n",
    "print(f\"Average episode return of {ave_returns/trails:3.2f}.\")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744cf2e",
   "metadata": {},
   "source": [
    "### Visualize the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07b70235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached episode return of 309.9099581697705.\n"
     ]
    }
   ],
   "source": [
    "# Create the RL environment to test against (same as was used for training earlier).\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\", gravity=-9.81, wind_power=15.0, enable_wind=False, continuous=True)\n",
    "\n",
    "episode_return = 0.0\n",
    "done = False\n",
    "\n",
    "# Reset the env to get the initial observation.\n",
    "obs, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    # Uncomment this line to render the env.\n",
    "    # env.render()\n",
    "\n",
    "    # Compute the next action from a batch (B=1) of observations.\n",
    "    obs_batch = torch.from_numpy(obs).unsqueeze(0)  # add batch B=1 dimension\n",
    "    model_outputs = rl_module.forward_inference({\"obs\": obs_batch})\n",
    "\n",
    "    # Extract the action distribution parameters from the output and dissolve batch dim.\n",
    "    action_dist_params = model_outputs[\"action_dist_inputs\"][0].numpy()\n",
    "\n",
    "    # We have continuous actions -> take the mean (max likelihood).\n",
    "    greedy_action_0 = np.clip(\n",
    "        action_dist_params[0],  # 0-1=mean, 2-3=log(stddev)\n",
    "        a_min=env.action_space.low[0],\n",
    "        a_max=env.action_space.high[0],\n",
    "    )\n",
    "    greedy_action_1 = np.clip(\n",
    "        action_dist_params[1],  # 0-1=mean, 2-3=log(stddev)\n",
    "        a_min=env.action_space.low[1],\n",
    "        a_max=env.action_space.high[1],\n",
    "    )\n",
    "    greedy_action = np.array([greedy_action_0, greedy_action_1])\n",
    "    # # For discrete actions, you should take the argmax over the logits:\n",
    "    # greedy_action = np.argmax(action_dist_params)\n",
    "\n",
    "    # Send the action to the environment for the next step.\n",
    "    obs, reward, terminated, truncated, info = env.step(greedy_action)\n",
    "\n",
    "    # Perform env-loop bookkeeping.\n",
    "    episode_return += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Reached episode return of {episode_return}.\")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
